{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline","metadata":{"id":"MkbavYMiRxWN","execution":{"iopub.status.busy":"2021-10-08T15:23:55.778885Z","iopub.execute_input":"2021-10-08T15:23:55.779675Z","iopub.status.idle":"2021-10-08T15:23:55.804785Z","shell.execute_reply.started":"2021-10-08T15:23:55.779571Z","shell.execute_reply":"2021-10-08T15:23:55.803715Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"\nThe eigenfaces example: chaining PCA and SVMs\n=============================================\n\nThe goal of this example is to show how an unsupervised method and a\nsupervised one can be chained for better prediction. \n\nHere we'll take a look at a simple facial recognition example. Ideally,\nwe would use a dataset consisting of a subset of the `Labeled Faces in\nthe Wild <http://vis-www.cs.umass.edu/lfw/>`__ data that is available\nwith :func:`sklearn.datasets.fetch_lfw_people`. The labelled face in the wild face dataset.\n\nHowever, this is a relatively large download (~200MB) so we will do the tutorial on a simpler, less rich dataset. \n\n","metadata":{"id":"fVxf5ue7RxW5"}},{"cell_type":"code","source":"from sklearn import datasets\nfaces = datasets.fetch_olivetti_faces()\nfaces.data.shape","metadata":{"id":"Ji8l6GbiRxW_","outputId":"69d4c146-8f31-4e5d-b878-253319849429","execution":{"iopub.status.busy":"2021-10-08T15:23:55.806670Z","iopub.execute_input":"2021-10-08T15:23:55.806899Z","iopub.status.idle":"2021-10-08T15:24:16.960460Z","shell.execute_reply.started":"2021-10-08T15:23:55.806875Z","shell.execute_reply":"2021-10-08T15:24:16.957836Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize these faces to see what we're working with\n\n","metadata":{"id":"La__4Oa0RxXo"}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfig = plt.figure(figsize=(8, 6))\n# plot several images\nfor i in range(15):\n    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n    ax.imshow(faces.images[i], cmap=plt.cm.bone)","metadata":{"id":"gyACcsH2RxXv","outputId":"8cd5cae3-deb2-454d-b4f4-d2902c3a324b","execution":{"iopub.status.busy":"2021-10-08T15:24:16.961463Z","iopub.status.idle":"2021-10-08T15:24:16.961866Z","shell.execute_reply.started":"2021-10-08T15:24:16.961650Z","shell.execute_reply":"2021-10-08T15:24:16.961674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note is that these faces have already been localized and scaled to a common size. \n   \nThis is an important preprocessing piece for facial recognition, and is a process that can require a large collection of training data. \n\nThis can be done in scikit-learn, but the challenge is gathering a sufficient amount of training data for the algorithm to work.\n\nWe'll perform a Support Vector classification of the images. We'll do a\ntypical train-test split on the images:\n\n","metadata":{"id":"Jn3nVfylRxYN"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(faces.data,\n        faces.target, random_state=0)\n\nprint(X_train.shape, X_test.shape)","metadata":{"id":"hDgt10c1RxYn","outputId":"e62fcaf1-d1a3-4832-d82d-116e5da304f7","execution":{"iopub.status.busy":"2021-10-08T15:24:16.963487Z","iopub.status.idle":"2021-10-08T15:24:16.963885Z","shell.execute_reply.started":"2021-10-08T15:24:16.963688Z","shell.execute_reply":"2021-10-08T15:24:16.963712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocessing: Principal Component Analysis\n-------------------------------------------\n\nWe can use PCA to reduce these features to a manageable size, while maintaining most of the information\nin the dataset.\n\n","metadata":{"id":"U_zhGfTaRxY-"}},{"cell_type":"code","source":"from sklearn import decomposition\npca = decomposition.PCA(n_components=150, whiten=True)\npca.fit(X_train)","metadata":{"id":"zGxiRg_qRxZA","outputId":"fc42cefa-b7f6-42a1-ea20-24e8e155c028","execution":{"iopub.status.busy":"2021-10-08T15:24:16.968300Z","iopub.status.idle":"2021-10-08T15:24:16.968689Z","shell.execute_reply.started":"2021-10-08T15:24:16.968453Z","shell.execute_reply":"2021-10-08T15:24:16.968478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One interesting part of PCA is that it computes the \"mean\" face, which\ncan be interesting to examine:\n\n","metadata":{"id":"Yhy0wHC5RxZT"}},{"cell_type":"code","source":"plt.imshow(pca.mean_.reshape(faces.images[0].shape),\n           cmap=plt.cm.bone)","metadata":{"id":"bwbJKHYXRxZY","outputId":"269822d0-0b5a-4963-8fa8-eb04d35a8f07","execution":{"iopub.status.busy":"2021-10-08T15:24:16.970208Z","iopub.status.idle":"2021-10-08T15:24:16.970622Z","shell.execute_reply.started":"2021-10-08T15:24:16.970410Z","shell.execute_reply":"2021-10-08T15:24:16.970433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The principal components measure deviations about this mean along\northogonal axes.\n\n","metadata":{"id":"oIvW5QygRxZv"}},{"cell_type":"code","source":"print(pca.components_.shape)","metadata":{"id":"LgOkmKjcRxZy","outputId":"fbaa2812-3e2f-4406-ccb7-0ba2601d8724","execution":{"iopub.status.busy":"2021-10-08T15:24:16.971648Z","iopub.status.idle":"2021-10-08T15:24:16.972748Z","shell.execute_reply.started":"2021-10-08T15:24:16.972465Z","shell.execute_reply":"2021-10-08T15:24:16.972493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is also interesting to visualize these principal components:\n\n","metadata":{"id":"mn8IC0lORxZ-"}},{"cell_type":"code","source":"fig = plt.figure(figsize=(16, 6))\nfor i in range(30):\n    ax = fig.add_subplot(3, 10, i + 1, xticks=[], yticks=[])\n    ax.imshow(pca.components_[i].reshape(faces.images[0].shape),\n              cmap=plt.cm.bone)","metadata":{"id":"9ePFr1ZARxaC","outputId":"e60d4fd4-3b99-406f-949b-4dafed9ea73f","execution":{"iopub.status.busy":"2021-10-08T15:24:16.973937Z","iopub.status.idle":"2021-10-08T15:24:16.974281Z","shell.execute_reply.started":"2021-10-08T15:24:16.974103Z","shell.execute_reply":"2021-10-08T15:24:16.974125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The components (\"eigenfaces\") are ordered by their importance from\ntop-left to bottom-right. We see that the first few components seem to\nprimarily take care of lighting conditions; the remaining components\npull out certain identifying features: the nose, eyes, eyebrows, etc.\n\nWith this projection computed, we can now project our original training\nand test data onto the PCA basis:\n\n","metadata":{"id":"SBkPczznRxaT"}},{"cell_type":"code","source":"X_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(X_train_pca.shape)","metadata":{"id":"0a8FG3HFRxaW","outputId":"edde5965-60fa-4f6a-bdbe-03e43866d52c","execution":{"iopub.status.busy":"2021-10-08T15:24:16.975243Z","iopub.status.idle":"2021-10-08T15:24:16.975587Z","shell.execute_reply.started":"2021-10-08T15:24:16.975399Z","shell.execute_reply":"2021-10-08T15:24:16.975420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_test_pca.shape)","metadata":{"id":"bwJ9tns3Rxar","outputId":"bc6b818d-9adb-4dae-d8be-9ccc19c8071c","execution":{"iopub.status.busy":"2021-10-08T15:24:16.976893Z","iopub.status.idle":"2021-10-08T15:24:16.977306Z","shell.execute_reply.started":"2021-10-08T15:24:16.977053Z","shell.execute_reply":"2021-10-08T15:24:16.977075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These projected components correspond to factors in a linear combination\nof component images such that the combination approaches the original\nface.\n\nDoing the Learning: Support Vector Machines\n-------------------------------------------\n\nNow we'll perform support-vector-machine classification on this reduced\ndataset:\n\n","metadata":{"id":"rit_f47IRxbC"}},{"cell_type":"code","source":"from sklearn import svm\nclf = svm.SVC(C=5., gamma=0.001)\nclf.fit(X_train_pca, y_train)","metadata":{"id":"jtFRD3fORxbH","outputId":"0a34e519-9476-40e5-d6fb-d5821fded5c7","execution":{"iopub.status.busy":"2021-10-08T15:24:16.978903Z","iopub.status.idle":"2021-10-08T15:24:16.979262Z","shell.execute_reply.started":"2021-10-08T15:24:16.979085Z","shell.execute_reply":"2021-10-08T15:24:16.979108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we can evaluate how well this classification did. First, we\nmight plot a few of the test-cases with the labels learned from the\ntraining set:\n\n","metadata":{"id":"7kvWiXT-Rxbf"}},{"cell_type":"code","source":"import numpy as np\nfig = plt.figure(figsize=(8, 6))\nfor i in range(15):\n    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n    ax.imshow(X_test[i].reshape(faces.images[0].shape),\n              cmap=plt.cm.bone)\n    y_pred = clf.predict(X_test_pca[i, np.newaxis])[0]\n    color = ('black' if y_pred == y_test[i] else 'red')\n    ax.set_title(y_pred, fontsize='small', color=color)","metadata":{"id":"Y7aXBtRKRxbp","outputId":"62d21908-ac9b-4639-c2cb-f458582a22c3","execution":{"iopub.status.busy":"2021-10-08T15:24:16.980137Z","iopub.status.idle":"2021-10-08T15:24:16.980500Z","shell.execute_reply.started":"2021-10-08T15:24:16.980310Z","shell.execute_reply":"2021-10-08T15:24:16.980331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classifier is correct on an impressive number of images given the\nsimplicity of its learning model! Using a linear classifier on 150\nfeatures derived from the pixel-level data, the algorithm correctly\nidentifies a large number of the people in the images.\n\nAgain, we can quantify this effectiveness using one of several measures\nfrom :mod:`sklearn.metrics`. First we can do the classification\nreport, which shows the precision, recall and other measures of the\n\"goodness\" of the classification:\n\n","metadata":{"id":"yFzw1hvKRxcB"}},{"cell_type":"code","source":"from sklearn import metrics\ny_pred = clf.predict(X_test_pca)\nprint(metrics.classification_report(y_test, y_pred))","metadata":{"id":"pIRPKH2jRxcE","outputId":"15576e80-5f65-435b-c596-a8e3bb31f15d","execution":{"iopub.status.busy":"2021-10-08T15:24:16.981703Z","iopub.status.idle":"2021-10-08T15:24:16.982031Z","shell.execute_reply.started":"2021-10-08T15:24:16.981849Z","shell.execute_reply":"2021-10-08T15:24:16.981870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another interesting metric is the *confusion matrix*, which indicates\nhow often any two items are mixed-up. The confusion matrix of a perfect\nclassifier would only have nonzero entries on the diagonal, with zeros\non the off-diagonal:\n\n","metadata":{"id":"jztgVH4fRxci"}},{"cell_type":"code","source":"print(metrics.confusion_matrix(y_test, y_pred))","metadata":{"id":"jPcMhOBoRxcl","outputId":"8ac9486d-2e4f-4f57-c2ec-b4c7c6e5a805","execution":{"iopub.status.busy":"2021-10-08T15:24:16.983034Z","iopub.status.idle":"2021-10-08T15:24:16.983403Z","shell.execute_reply.started":"2021-10-08T15:24:16.983211Z","shell.execute_reply":"2021-10-08T15:24:16.983241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pipelining\n----------\n\nAbove we used PCA as a pre-processing step before applying our support\nvector machine classifier. Plugging the output of one estimator directly\ninto the input of a second estimator is a commonly used pattern; for\nthis reason scikit-learn provides a ``Pipeline`` object which automates\nthis process. The above problem can be re-expressed as a pipeline as\nfollows:\n\n","metadata":{"id":"f8aB9bwGRxc_"}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nclf = Pipeline([('pca', decomposition.PCA(n_components=150, whiten=True)),\n                ('svm', svm.LinearSVC(C=1.0))])\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(metrics.confusion_matrix(y_pred, y_test))\nplt.show()","metadata":{"id":"Wei0jY67RxdC","outputId":"f2926d52-9a7c-40bb-827d-87418acc9475","execution":{"iopub.status.busy":"2021-10-08T15:24:16.984503Z","iopub.status.idle":"2021-10-08T15:24:16.984915Z","shell.execute_reply.started":"2021-10-08T15:24:16.984731Z","shell.execute_reply":"2021-10-08T15:24:16.984754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A Note on Facial Recognition\n----------------------------\n\nHere we have used PCA \"eigenfaces\" as a pre-processing step for facial\nrecognition. The reason we chose this is because PCA is a\nbroadly-applicable technique, which can be useful for a wide array of\ndata types. Research in the field of facial recognition in particular,\nhowever, has shown that other more specific feature extraction methods\nare can be much more effective.\n\n","metadata":{"id":"3K-HzY4fRxdT"}},{"cell_type":"markdown","source":"Assignment: Perform SVM with PCA operation on Breast Cancer Dataset and Iris Dataset.","metadata":{"id":"VJj6B4c4I1Bz"}},{"cell_type":"code","source":"","metadata":{"id":"uCtx1NGeJFmA"},"execution_count":null,"outputs":[]}]}