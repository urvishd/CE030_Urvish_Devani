# -*- coding: utf-8 -*-
"""lab5_0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ekot9Z57cXfte5uO0zmX-cGQ4w7DLMif
"""

# Import Numpy & PyTorch
import numpy as np
import torch
from sklearn.linear_model import LinearRegression

"""A tensor is a number, vector, matrix or any n-dimensional array.



It means that the yield of apples is a linear or planar function of the temperature, rainfall & humidity.



**Our objective**: Find a suitable set of *weights* and *biases* using the training data, to make accurate predictions.

## Training Data
The training data can be represented using 2 matrices (inputs and targets), each with one row per observation and one column for variable.
"""

# Input (temp, rainfall, humidity)
inputs = np.array([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70]], dtype='float32')

# Target (apples)
targets = np.array([[56], 
                    [81], 
                    [119], 
                    [22], 
                    [103]], dtype='float32')

"""Before we build a model, we need to convert inputs and targets to PyTorch tensors."""

# Convert inputs and targets to tensors
input_tensor = torch.tensor(inputs,requires_grad=True)
print(input_tensor)
target_tensor= torch.tensor(targets,requires_grad=True)
print(target_tensor)

"""## Linear Regression Model (from scratch)

The *weights* and *biases* can also be represented as matrices, initialized with random values. The first row of `w` and the first element of `b` are use to predict the first target variable i.e. yield for apples, and similarly the second for oranges.
"""

# Weights and biases
w = np.array([0.2, 0.5,0.3])
b = 0.1

y = np.matmul(w, input_tensor.detach().numpy().T) + b





def mse(t1, t2):
    diff = t1 - t2
    return np.sum(diff * diff) / diff.size

# Compute loss
preds = model(input_tensor.detach().numpy(),w)
cost_initial = mse(preds, target_tensor.detach().numpy())
print("Cost before regression: ",cost_initial)

"""The resulting number is called the **loss**, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model.

## Compute Gradients

With PyTorch, we can automatically compute the gradient or derivative of the `loss` w.r.t. to the weights and biases, because they have `requires_grad` set to `True`.

More on autograd:  https://pytorch.org/docs/stable/autograd.html#module-torch.autograd
"""

# Compute gradients
def model(x,w):
    return x @ w.T

def gradient_descent(X, y, w, learning_rate, n_iters):
    J_history = np.zeros((n_iters,1))
    for i in range(n_iters):
        h = model(X,w)
        diff = h - y
        delta = (learning_rate/y.size)*(X.T@diff)
        new_w = w - delta.T
        w=new_w
        J_history[i] = mse(h, y)
    return (J_history, w)

"""## Train for multiple epochs

To reduce the loss further, we repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an epoch.
"""

# Train for 100 epochs
import matplotlib.pyplot as plt
n_iters = 500
learning_rate = 0.01

initial_cost = mse(model(input_tensor.detach().numpy(),w),target_tensor.detach().numpy())

print("Initial cost is: ", initial_cost, "\n")

(J_history, optimal_params) = gradient_descent(input_tensor.detach().numpy(), target_tensor.detach().numpy(), w, learning_rate, n_iters)

print("Optimal parameters are: \n", optimal_params, "\n")

print("Final cost is: ", J_history[-1])

plt.plot(range(len(J_history)), J_history, 'r')

plt.title("Convergence Graph of Cost Function")
plt.xlabel("Number of Iterations")
plt.ylabel("Cost")
plt.show()

# Calculate error
preds = model(X,optimal_params)
cost_final = mse(preds, Y)
# Print predictions
print("Prediction:\n",preds)
# Comparing predicted with targets
print("Targets:\n",Y)

print("Cost after linear regression: ",cost_final)
print("Cost reduction percentage : {} %".format(((cost_initial- cost_final)/cost_initial)*100))